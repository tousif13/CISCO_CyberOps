# 27.1.5 Lab - Convert Data into a Universal Forma

## Part 1: Normalize Timestamps in a Log File

* Launch the CyberOps Workstation VM and then launch a terminal window
* Use the cd command to change to the /home/analyst/lab.support.files/ directory. A copy of the file shown above is stored there

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/137ea459-03a0-4362-aeff-f6fae2149088)

* Issue the following AWK command to convert and print the result on the terminal:
* `awk 'BEGIN {FS=OFS="|"}{$3=strftime("%c",$3)} {print}' applicationX_in_epoch.log`

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/1afe02e6-f506-42c4-969e-851a7d627b68)

    awk – This invokes the AWK interpreter.
    ‘BEGIN – This defines the beginning of the script.
    {} – This defines actions to be taken in each line of the input text file. An AWK script can have several actions.
    FS = OFS = “|” – This defines the field separator (i.e., delimiter) as the bar (|) symbol. Different text files may use different delimiting characters to separate fields. This operator allows the user to define what character is used as the field separator in the current text file.
    $3 – This refers to the value in the third column of the current line. In the applicationX_in_epoch.log, the third column contains the timestamp in epoch to be converted.
    strftime - This is an AWK internal function designed to work with time. The %c and $3 in between parenthesis are the parameters passed to strftime.
    applicationX_in_epoch.log – This is the input text file to be loaded and used. Because you are already in the lab.support.files directory, you do not need to add path information, /home/analyst/lab.support.files/applicationX_in_epoch.log.

* Were the Unix Epoch timestamps converted to Human Readable format? Were the other fields modified? Explain.

      Yes, the unix epoch timestamps converted to human readable format and other fields are not modified as above command modifies only timestamps.

* Compare the contents of the file and the printed output. Why is there the line, ||Wed 31 Dec 1969 07:00:00 PM EST?

      The reason for the extra line is because the file has an empty line at the end, which led the script to mistakenly interpret it as 0 and convert that into a Human Readable timestamp.

* Use nano (or your favorite text editor) to remove the extra empty line at the end of the file and run the AWK script again by using the up-arrow to find it in the command history buffer.

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/1f4f8cda-cdfe-4fb2-8b35-8b7411ad4e78)

* Is the output correct now? Explain

      Yes, as last line is removed. Now the output is correct.

* While printing the result on the screen is useful for troubleshooting the script, analysts will likely need to save the output in a text file. Redirect the output of the script above to a file named `applicationX_in_human.log` to save it to a file:

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/6dd51646-27a7-4837-87e7-639dc9466065)

* What was printed by the command above? Is this expected?

        Nothing printed. Output redirected to the given file.

* Use `cat` to view the applicationX_in_human.log. Notice that the extra line is now removed and the timestamps for the log entries have been converted to human readable format.

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/f568fb45-92e8-4737-8dbb-fb503446a15e)

## Part 2: Normalize Timestamps in an Apache Log File

* Similar to what was done with the applicationX_in_epoch.log file, Apache web server log files can also be normalized. Follow the steps below to convert Unix Epoch to Human Readable timestamps. Consider the following Apache log file, apache_in_epoch.log:

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/286b089a-a08d-4ea5-a009-6755b4d72e00)

* As in Part 1, a script will be created to convert the timestamp from Epoch to Human Readable.
* In the context of timestamp conversion, what character would work as a good delimiter character for the Apache log file above?

        Space character

* How many columns does the Apache log file above contain?

          7

* In the Apache log file above, what column contains the Unix Epoch Timestamp?

          Column 4

* In the CyberOps Workstation VM terminal, a copy of the Apache log file, apache_in_epoch.log, is stored in the /home/analyst/lab.support.files.
* Use an awk script to convert the timestamp field to a human readable format. Notice that the command contains the same script used previously, but with a few adjustments for the delimiter, timestamp field,and file name.

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/7870f3c3-8e25-4432-b68c-f51d9a04fa93)

* Was the script able to properly convert the timestamps? Describe the output

          No. All timestamps are displayed as Wed 31 Dec 1969 07:00:00 PM EST.

* Before moving forward, think about the output of the script.
* Can you guess what caused the incorrect output? Is the script incorrect? What are the relevant differences between the applicationX_in_epoch.log and apache_in_epoch.log?

          Because of '[' character, the script can't convert [ as a timestamp. Thus assuming it as zero it displays incorrect output.

* To fix the problem, the square brackets must be removed from the timestamp field before the conversion takes place. Adjust the script by adding two actions before the conversion, as shown below:
* `awk 'BEGIN {FS=OFS=" "}{gsub(/\[|\]/,"",$4)}{print}{$4=strftime("%c",$4)}{print}' apache_in_epoch.log`

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/1f61befb-6f61-49b3-85dd-b4bf84b0cf4a)

* Notice after specifying space as the delimiter with `{FS=OFS=” “}`, there is a regular expression action to match and replace the square brackets with an empty string, effectively removing the square brackets that appear in the timestamp field. The second action prints the updated line so the conversion action can be performed
* In a CyberOps Workstation VM terminal, execute the adjusted script, as follows:

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/b5e642ad-a0bc-4d37-93dd-871ea4d976d4)

* Was the script able to properly convert the timestamps this time? Describe the output.

          Yes. The output now displays two lines for each log entry. The first line displays using Human Readable format and the second line is the same log entry with the timestamp displaye the Unix Epoch format

* Shut down CyberOps Workstation VM if desired

## Part 3: Log File Preparation in Security Onion Virtual Machine

> Because log file normalization is important, log analysis tools often include log normalization features. Tools 
that do not include such features often rely on plugins for log normalization and preparation. The goal of these 
plugins is to allow log analysis tools to normalize and prepare the received log files for tool consumption.
The Security Onion appliance relies on a number of tools to provide log analysis services. ELK, Zeek, Snort
and SGUIL are arguably the most used tools

### Step 1: Start Security Onion VM

* Launch the Security Onion VM from VirtualBox’s Dashboard (username: analyst / password: cyberops)

### Step 2: Zeek Logs in Security Onion

* Open a terminal window in the Security Onion VM. Right-click the Desktop. In the pop-up menu, select Open Terminal
* Zeek logs are stored at `/nsm/bro/logs/`. As usual with Linux systems, log files are rotated based on the date, renamed and stored on the disk. The current log files can be found under the current directory. From the terminal window, change directory using the following command. (`cd /nsm/bro/logs/current`)
* Use the ls -l command to see the log files generated by Zeek

  ![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/99f9a37c-9629-4e99-8739-f34b6e5c08cc)

* `Note` : Depends on the state of the virtual machine, there may not be any log files yet.

### Step 3: Snort Logs

* Snort logs can be found at `/nsm/sensor_data/.` Change directory as follows.
* Use the ls -l command to see all the log files generated by Snort.

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/cd3e04f1-ad68-44fd-9223-1375f3857f6e)

* Notice that Security Onion separates files based on the interface. Because the `Security Onion VM` image has two interfaces configured as sensors and a special folder for imported data, three directories are kept. Use the `ls –l seconion-eth0` command to see the files generated by the eth0 interface

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/e026df85-3c0b-4897-ba8c-fc085d2e430c)

### Step 4: Various Logs

* While the `/nsm/` directory stores some log files, more specific log files can be found under `/var/log/nsm/.` Change directory and use the ls command to see all the log files in the directory.

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/34d40723-f0aa-4f9f-ad11-8f07d4626ed6)

* Notice that the directory shown above also contains logs used by secondary tools such as `OSSEC` and `Squert`.
* ELK logs can be found in the `/var/log` directory. Change directory and use the ls command to list the files and directories.

![image](https://github.com/tousif13/CISCO_CyberOps/assets/33444140/44c5340f-620e-4837-b081-dd73cab3ffbf)

* Take some time to Google these secondary tools and answer the questions below:
* For each one of the tools listed above, describe the function, importance, and placement in the security analyst workflow

        Pulledpork is a Snort rule manage system. It facilitates Snort rules updating. Outdated Snort rules makes the entire system useless.

        OSSEC is a system used to normalize and concentrate local system logs. When deployed throughout the organization, OSSEC allows an analyst to have a clear picture of what is happening in the systems.

        Squert is a visual tool that attempts to provide additional context to events through the use of metadata, time series representations, and weighted and logically grouped result sets.

          Elasticsearch serves as a powerful tool for ingesting, storing, searching, and analyzing security-related data, including logs and events. It is essential for real-time data analysis, scalability to handle large data volumes, data correlation, historical analysis, and creating visualizations for informed decision-making. It is positioned at the core of the security analyst workflow, facilitating data collection, analysis, alerting, visualization, and historical analysis to ensure effective threat detection and response.

          Kibana is a data visualization and exploration tool that complements Elasticsearch. It allows security analysts to create interactive dashboards, perform ad-hoc data exploration, and generate visual representations of security data, making it easier to monitor, analyze, and communicate findings. It  is crucial for security analysts as it translates raw data stored in Elasticsearch into visual insights, enabling better data interpretation, trend identification, and rapid decision-making. It enhances the accessibility of data for non-technical stakeholders. It is typically used in the latter stages of the security analyst workflow, after data is ingested and analyzed using Elasticsearch. It's employed for creating dashboards, visualizing data, and reporting findings, enhancing the communication of security insights.

           Logstash is a data processing tool that collects, parses, and enriches logs and other event data from various sources, making it suitable for analysis. In the security analyst workflow, Logstash functions as an ETL (Extract, Transform, Load) tool, preparing data for storage and analysis in Elasticsearch. It enables security analysts to transform and structure incoming data, making it suitable for storage and analysis in Elasticsearch. It is typically positioned at the beginning of the security analyst workflow. It collects data from various sources, performs transformations to standardize and enrich it, and then sends the processed data to Elasticsearch for storage and analysis.

## Reflection

* Log normalization is important and depends on the deployed environment.
* Popular tools include their own normalization features, but log normalization can also be done manually.
* When manually normalizing and preparing log files, double-check scripts to ensure the desired result is achieved. A poorly written normalization script may modify the data, directly impacting t he analyst’s work
